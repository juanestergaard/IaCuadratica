# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AhnVucRJQAAaWlKxR2Z98PRvIb1am0Ly
"""

#1
#Primero, importamos las librerías que necesitamos:
import tensorflow as tf
import numpy as np
from tensorflow import keras
import matplotlib.pyplot as plt

#import numpy as np

#2
#Luego, generamos nuestros datos de entrenamiento:
x_train = np.linspace(-1, 1, 20)
y_train = x_train ** 2 +x_train + 3

#t1=np.array([100,120,140,160,180,200,220,240,260,280,300,320,340,360,380,400,420,440,460,480,500],dtype=float)
#t2=np.array([100,120,140,160,180,200,220,240,260,280,300,320,340,360,380,400,420,440,460,480,500],dtype=float)

#3
#Definimos nuestra red neuronal con una capa oculta y una neurona de salida:
model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)), #10 neurnoas
  tf.keras.layers.Dense(10, activation='relu'),#agregada
  tf.keras.layers.Dense(1, activation='relu')# función de activación ReLU en la capa de salida
])

#sigmoid: La función sigmoide es una función no lineal que comprime los valores en el rango [0,1]. Es muy útil para clasificación binaria, ya que produce una salida que se puede interpretar como la probabilidad de pertenecer a la clase positiva. Sin embargo, la función sigmoide puede sufrir de "desvanecimiento del gradiente" en redes neuronales profundas.

#tanh: La función tangente hiperbólica es una función no lineal que comprime los valores en el rango [-1,1]. Es similar a la función sigmoide, pero tiene un rango de salida simétrico alrededor de cero. También puede sufrir de desvanecimiento del gradiente en redes neuronales profundas.

#ReLU (Rectified Linear Unit): La función ReLU es una función no lineal que mantiene los valores positivos sin cambios y establece los valores negativos en cero. Es muy popular en redes neuronales debido a su simplicidad y eficiencia en el cálculo de los gradientes. Sin embargo, puede sufrir de "neuronas muertas" si se establece un valor de umbral muy bajo.

#LeakyReLU (Leaky Rectified Linear Unit): La función Leaky ReLU es similar a la función ReLU, pero en lugar de establecer los valores negativos en cero, establece los valores negativos en una pequeña pendiente. Esto evita el problema de las neuronas muertas en la función ReLU.

#softmax: La función softmax es una función no lineal que se utiliza comúnmente en la capa de salida de una red neuronal para clasificación multiclase. Produce una distribución de probabilidad sobre todas las clases.

#ELU (Exponential Linear Unit): La función ELU es similar a la función ReLU, pero utiliza una función exponencial para los valores negativos. Esto proporciona una mejora en el rendimiento sobre la función ReLU en algunas situaciones.

#Swish: La función Swish es similar a la función sigmoid, pero utiliza la entrada multiplicada por la función sigmoid. La función Swish puede proporcionar una mejora en el rendimiento sobre la función ReLU en algunas situaciones.

#Para utilizar estas funciones de activación en tu modelo, simplemente reemplaza 'relu' en la definición de la capa con la función de activación deseada. Por ejemplo, 'sigmoid', 'tanh', 'softmax', etc.

#Espero que esto te ayude a entender mejor las diferentes funciones de activación y sus diferencias.


#t1

#4
# Compilar el modelo
model.compile(optimizer='adam', loss='mse')
#Después de definir la arquitectura de la red neuronal, la siguiente línea de código compila el modelo utilizando el optimizador Adam y la función de pérdida MSE (mean squared error). Adam es un optimizador muy utilizado en redes neuronales y MSE se utiliza a menudo como función de pérdida para problemas de regresión.
#Compilamos el modelo, especificando la función de pérdida y el optimizador:
#model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=0.05))


#at1=t1.reshape(21,1)
#print(at1)

#5
#Entrenamos el modelo durante 100 épocas:
history = model.fit(x_train, y_train, epochs=300, verbose=0)
#Imprimimos el error de las últimas 5 épocas
last_losses = history.history['loss'][-5:]
print("Últimas 5 épocas de error: ", last_losses)

#at2=t2.reshape(21,1)

#6
# Graficar la curva de aprendizaje
plt.plot(history.history['loss'])
plt.title('Curva de aprendizaje')
plt.ylabel('Pérdida')
plt.xlabel('Época')
plt.show()

#graficamos los datos de entrenamiento y la predicción del modelo:
x_test = np.linspace(-1, 1, 100)
y_pred = model.predict(x_test)

plt.plot(x_train, y_train, 'o', label='Datos de entrenamiento')
plt.plot(x_test, y_pred, label='Predicción del modelo')
plt.legend()
plt.show()
#El resultado será una gráfica que muestra los datos de entrenamiento como puntos y la predicción del modelo como una línea curva que se ajusta a los puntos. Aquí está el código completo:

#mm=np.zeros((441,6))
#mm

#7
#model.save('modelo.h5')
#from keras.models import load_model

#model = load_model('modelo.h5')

x_new = 1
y_new = model.predict([x_new])
print("La predicción para x =", x_new ,"es:", y_new[0][0])

"""
j=0  
k=0
for i in range(441):
  mm[i,0]=(at1[j,0])
  mm[i,1]=(at2[k,0])
  j=j+1
  if j == 21:
   j=0
   k=k+1
print(mm)
"""

#8
#for i in range(441):
  #mm[i,2]=((100000*((mm[i,0])-100))/(120*(300-(mm[i,0]))))
  #mm[i,3]=((100000*((mm[i,1])-(mm[i,0])))/(80*(400-(mm[i,1]))))
  #mm[i,4]=((100000*(500-(mm[i,1])))/(40*(600-500)))

#9
#print(mm)

#10
"""
for i in range(441):
  mm[i,5]=((mm[i,2])+(mm[i,3])+(mm[i,4]))
  """

#11
#print(mm)

#12
"""
minnn=mm[0,5]
superi=0
for i in range(441):
  if (mm[i,2])>0:
    if (mm[i,3])>0:
      if (mm[i,4])>0:
        if (mm[i,5])<minnn:
          minnn=(mm[i,5])
          superi=i
print(minnn)
print(superi)
"""

#13
#print(mm[superi,:])

#14

'''
Conjunto de funciones desarrolladas para ser utilizadas en los programas 
principales
'''
"""
import numpy as np
import torch
import matplotlib.pyplot as plt
!pip install labcomdig
from labcomdig import gray2de
from network import *
"""
'''
NUMPY
'''

#15
#!pip install labcomdig
#import labcomdig

